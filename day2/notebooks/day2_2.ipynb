{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c04da39",
   "metadata": {},
   "source": [
    "# Day 2-Part 2: Reading and visualising data\n",
    "\n",
    "We have already used the `numpy.loadtxt` function to load some data (the vertices of a polygon) from a text file. This function offers a lot of flexibility in the way we can read data from a text file by specifying options such as the data type (e.g., float or string), delimiter (the string used to separate values), skipping some lines (e.g., headers), using some columns, etc. You can find more about this in the [loadtxt documentation](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html). \n",
    "\n",
    "However, when working with more complex data such as data stored in spreadsheets or databases, the [`pandas`](https://pandas.pydata.org) (Python data analysis) library is the preferred choice. `pandas` will not only allow you to load and plot data, but as we will see throughout the course, it is great to explore and operate on data. Data analysis boils down to üêº.\n",
    "\n",
    "This notebook illustrates the use of `pandas` for reading and visualising data via two examples. Before we cover the examples, let's take a look at the core data structure of `pandas`, namely the `DataFrame`:\n",
    "\n",
    "## DataFrame\n",
    "\n",
    "A `DataFrame` is a two-dimensional object where data are stored in column and row format. Suppose we have the following table, and we want to make it into a `DataFrame`:\n",
    "\n",
    "<img src=\"../figures/minerals.png\" alt=\"minerals\" width=\"400\"/><br><br>\n",
    "\n",
    "We can do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas a pd\n",
    "\n",
    "df = pd.DataFrame() # create an empty DataFrame\n",
    "\n",
    "# add columns to the DataFrame using lists\n",
    "# note that lists/columns should have the same length\n",
    "df[\"mineral\"] = [\"halite\", \"quartz\", \"hematite\", \"rutile\", \"olivine\"]\n",
    "df[\"hardness\"] = [2.5, 7, 6, 6, 7]\n",
    "df[\"density\"] = [2.17, 2.65, 5.3, 4.24, 3.4]\n",
    "\n",
    "print(df) # view DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756864aa",
   "metadata": {},
   "source": [
    "Here, we created the `DataFrame` using lists. We can also create the DataFrame using a `Dictionary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554760aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dictionary\n",
    "my_dict = {\"mineral\":[\"halite\", \"quartz\", \"hematite\", \"rutile\", \"olivine\"],\n",
    "          \"hardness\":[2.5, 7, 6, 6, 7], \"density\":[2.17, 2.65, 5.3, 4.24, 3.4]}\n",
    "\n",
    "# create DataFrame using Dictionary\n",
    "df = pd.DataFrame(my_dict)\n",
    "\n",
    "print(df) # view DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b5318",
   "metadata": {},
   "source": [
    "In this case the `Dictionary` keys are the columns, and the values are the entries of the columns. What type of object is a `DataFrame` column? Let's look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df[\"mineral\"]), type(df[\"hardness\"]), type(df[\"density\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0db6c3",
   "metadata": {},
   "source": [
    "The columns are of type `Series`. Series are more flexible than Lists, and as opposed to Lists, it is possible to operate on them, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46679d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hardness/density\"] = df[\"hardness\"] / df[\"density\"] # add a column of hardness/density\n",
    "print(df) # print DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ea246",
   "metadata": {},
   "source": [
    "We will use this powerful functionality throughout the course. Now that we know what a `DataFrame` is, we can use it to read and visualize data in the following two examples:\n",
    "\n",
    "## Example 1: Worldwide earthquakes\n",
    "\n",
    "The Excel file `eq_stat.xlsx` in the `data` directory includes the M > 5 earthquakes worldwide from the [USGS earthquake statistics](https://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics) site. Let's analyse these data using `pandas`.\n",
    "\n",
    "First, we need to load the data. This is as easy as reading the file using the `pandas.read_excel` function. This loads the table as a `DataFrame` object, a two-dimensional labeled data structure with different columns. We can use the `DataFrame.head` method to display the first 5 (this is the default) rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # import operating system module\n",
    "\n",
    "path = os.path.join(\"..\", \"data\", \"eq_stat.xlsx\") # a safe path to the file\n",
    "eq_stat = pd.read_excel(path) # read earthquakes statistics\n",
    "eq_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227853b",
   "metadata": {},
   "source": [
    "The leftmost bold integers are the indexes of the rows. Every row in a dataframe has an index, and in this case the indexes of the rows happen to be integers.\n",
    "\n",
    "The first column are the ids of the rows (earthquake magnitudes and deaths), and the other columns are these data over the years 2000 to 2021. However, rather than having this information on a row basis, we would like to have it on a column basis. Let's use `pandas`to format the data as we want. We start by removing the 'Magnitude' column using the `DataFrame.drop` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_stat = eq_stat.drop(columns=[\"Magnitude\"]) # remove Magnitude column\n",
    "eq_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d30676",
   "metadata": {},
   "source": [
    "Then we make the rows into columns using the `DataFrame.transpose` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_stat = eq_stat.transpose() # make the rows into columns\n",
    "eq_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e9530",
   "metadata": {},
   "source": [
    "Notice that now the indexes of the rows are the years. Finally, we reorganize the columns such that the lowest magnitude earthquakes (5-5.9) are in the first column, followed by the higher magnitude earthquakes in the next columns. In the last column, we still keep the deaths. We also rename the columns to the original id descriptions, magnitudes and deaths, using the `DataFrame.rename` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8af191",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [3, 2, 1, 0, 4] # make a list of new columns\n",
    "eq_stat = eq_stat[new_cols] # reorganize columns\n",
    "eq_stat = eq_stat.rename(columns= {3:\"5-5.9\", 2:\"6-6.9\", 1:\"7-7.9\", 0:\"8.0+\", 4:\"Deaths\"}) # rename columns\n",
    "eq_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0791a",
   "metadata": {},
   "source": [
    "And we can also use the `DataFrame.tail` method to see the last 5 (this is the default) rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_stat.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af6dda",
   "metadata": {},
   "source": [
    "Notice that for the last two years, there is no information about Deaths. \n",
    "\n",
    "We are now ready to plot these data. We can use the `matplot.pyplot` module to do so. In the `plot` calls, the `DataFrame.iloc` function allows us to extract the desired entries by row and column indexes (for example, `iloc[:,0]` is the first column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc117f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import pyplot module\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(eq_stat.iloc[:,0], \".-\", color=\"dodgerblue\", label=eq_stat.columns[0]) # plot 5-5.9 earthquakes\n",
    "ax.plot(eq_stat.iloc[:,1], \"g.-\", label=eq_stat.columns[1]) # plot 6-6.9 earthquakes\n",
    "ax.plot(eq_stat.iloc[:,2], \".-\", color=\"orange\", label=eq_stat.columns[2]) # plot 7-7.9 earthquakes\n",
    "ax.plot(eq_stat.iloc[:,3], \"r.-\", label=eq_stat.columns[3]) # plot 8.0+ earthquakes\n",
    "\n",
    "ax.set_xlabel(\"year\") # set x label\n",
    "ax.set_ylabel(\"number of earthquakes\") # set y label\n",
    "ax.set_xlim([2000, 2021]) # set x limits\n",
    "my_xticks = [2000, 2005, 2010, 2015, 2020] # make a list of x ticks\n",
    "ax.set_xticks(my_xticks) # set x ticks\n",
    "ax.set_ylim([0, 3000]) # set y limits\n",
    "ax.legend(); # include legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a5a2",
   "metadata": {},
   "source": [
    "This is fine. However, we can plot these data in a much simpler way using the `DataFrame.plot` function. This is a wrapper of the `pyplot.plot` function in `pandas`. To do that, we first make a new DataFrame without the Deaths column. Then, we use this DataFrame to plot the earthquakes in just one line using the `DataFrame.plot` function. Notice that in this line we retrieve an axis object (`ax`) to latter set the properties of the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_stat_mod = eq_stat.drop(columns=[\"Deaths\"]) # new DataFrame without Deaths column\n",
    "\n",
    "# plot earthquakes\n",
    "ax = eq_stat_mod.plot(color={\"5-5.9\":\"dodgerblue\", \"6-6.9\":\"g\", \"7-7.9\":\"orange\", \"8.0+\":\"r\"}, style=\".-\")\n",
    "\n",
    "ax.set_xlabel(\"year\") # set x label\n",
    "ax.set_ylabel(\"number of earthquakes\") # set y label\n",
    "ax.set_xlim([0, 21]) # set x limits\n",
    "ax.set_ylim([0, 3000]); # set y limits, the legend is included by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595a265",
   "metadata": {},
   "source": [
    "This plot is okay but is not exactly what we want. It would be better to vertically stack the earthquakes in a chart area graph, to clearly display the variation of earthquakes through time. Conveniently, the `DataFrame.plot.area` function does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot earthquakes chart area\n",
    "ax = eq_stat_mod.plot.area(color={\"5-5.9\":\"dodgerblue\", \"6-6.9\":\"g\", \"7-7.9\":\"orange\", \"8.0+\":\"r\"}, \n",
    "                           legend=\"reverse\")\n",
    "\n",
    "ax.set_xlabel(\"year\") # set x label\n",
    "ax.set_ylabel(\"number of earthquakes\") # set y label\n",
    "ax.set_xlim([0, 21]) # set x limits\n",
    "ax.set_ylim([0, 3000]); # set y limits\n",
    "ax.set_title(\"Worlwide earthquakes\"); # set title of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf723d4",
   "metadata": {},
   "source": [
    "This looks much better. Let's add to this graph the number of earthquake related deaths. We can do this by adding another axis to the graph using the `ax.twinx` function. We then use this new axis to plot the Deaths column of the original `eq_stat` DataFrame. Notice that this time we use the `DataFrame.loc` function to retrieve the entries by their rows and columns labels. Also, we don't include the last two years of the `Deaths` column, because there are no entries for those years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# plot earthquakes chart area and set graph properties\n",
    "ax1 = eq_stat_mod.plot.area(color={\"5-5.9\":\"dodgerblue\", \"6-6.9\":\"g\", \"7-7.9\":\"orange\", \"8.0+\":\"r\"}, \n",
    "                            xlabel=\"year\", ylabel=\"number of earthquakes\", xlim=([0, 21]), \n",
    "                            ylim=([0, 3000]), legend=\"reverse\", \n",
    "                            title=\"Worlwide earthquakes and related deaths\")\n",
    "\n",
    "ax2 = ax1.twinx() # add new axis sharing the x axis\n",
    "ax2.plot(np.arange(0,20,1),eq_stat.loc[2000:2019,\"Deaths\"],\"ko-\") # plot Deaths on new axis\n",
    "ax2.set_ylabel(eq_stat.columns[4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbaa9c6",
   "metadata": {},
   "source": [
    "As you probably suspected, there is no correlation whatsoever between the number of earthquakes and the number of deaths in a year. Seismic hazards are much more complex than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214b7d6",
   "metadata": {},
   "source": [
    "## Example 2: Trace-elements in tephra samples \n",
    "\n",
    "This example is from the book [Introduction to Python in Earth Science Data Analysis](https://link.springer.com/book/10.1007/978-3-030-78055-5) by Maurizio Petrelli. The Excel file `Smith_glass_post_NYT_data.xlsx` contains the chemical concentrations of trace elements of volcanic ash (tephra) from the Campi Flegrei volcano, Italy ([Smith et al., 2011](https://doi.org/10.1016/J.QUASCIREV.2011.07.012)). Let's read the `Supp_traces` sheet from this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea339e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\", \"data\", \"Smith_glass_post_NYT_data.xlsx\") # a safe path\n",
    "my_dataset = pd.read_excel(path, sheet_name=\"Supp_traces\")\n",
    "my_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c42be",
   "metadata": {},
   "source": [
    "Not all the columns of the table are listed. To get a complete list of the columns, you can use the `list` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbc95d",
   "metadata": {},
   "source": [
    "And to quickly get a descriptive statistics of the dataset, you can use the `DataFrame.describe` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a37eb",
   "metadata": {},
   "source": [
    "Let's plot the elements Zr versus Th in a scatter diagram. This is done by selecting and plotting the Zr and Th columns, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf98942",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_dataset.Zr # select Zr column, my_dataset[\"Zr\"] also works\n",
    "y = my_dataset.Th # select Th column, my_dataset[\"Th\"] also works\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_title(\"Zr vs. Th\")\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Th [ppm]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587736f",
   "metadata": {},
   "source": [
    "Now, suppose that we want to define two datasets based on the concentration of Zr, one dataset with Zr >= 450 ppm, and another with Zr < 450 ppm. In addition we want to plot these datasets with different colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f452ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two sub-datasets for Zr >= 450 and Zr < 450\n",
    "my_sub_dataset1 = my_dataset[x >= 450] # sub_dataset with Zr >= 450 ppm\n",
    "my_sub_dataset2 = my_dataset[x < 450] # sub_dataset with Zr >= 450 ppm\n",
    "\n",
    "# generate the scatter Zr Vs Th diagram for Zr >= 450\n",
    "# in blue, also defining the legend caption as \"Zr >= 450 [ppm]\"\n",
    "x1 = my_sub_dataset1.Zr\n",
    "y1 = my_sub_dataset1.Th\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1, y1, color=\"blue\", label=\"Zr >= 450 [ppm]\")\n",
    "\n",
    "# generate the scatter Zr Vs Th diagram for Zr < 450\n",
    "# in red, also defining the legend caption as \"Zr < 450 [ppm]\"\n",
    "x2 = my_sub_dataset2.Zr\n",
    "y2 = my_sub_dataset2.Th\n",
    "ax.scatter(x2, y2, color=\"red\", label=\"Zr < 450 [ppm]\")\n",
    "\n",
    "ax.set_title(\"Zr vs Th\")\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Th [ppm]\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3057e2",
   "metadata": {},
   "source": [
    "Let's filter and plot the data by the labels in the `Epoch` column. To do this, we just make a list of epochs, and then using a `for` loop, we iterate over all epochs to filter and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f77f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [\"one\",\"two\",\"three\",\"three-b\"] # list of epochs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# filter and plot the dataset by epochs\n",
    "for epoch in epochs: # for each epoch\n",
    "    my_data = my_dataset[my_dataset.Epoch == epoch] # data belonging to epoch\n",
    "    ax.scatter(my_data.Zr, my_data.Th, label=\"Epoch \" + epoch) # plot data belonging to epoch\n",
    "\n",
    "ax.set_title(\"Zr vs Th\")\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Th [ppm]\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62924f7",
   "metadata": {},
   "source": [
    "Let's make a more fancy, publication ready plot. To do this, we define in addition colors and markers lists for the different epochs, and include those in the `for` loop. The `zip` function allows us to iterate over the three lists at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd36b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a publication ready plot\n",
    "epochs = [\"one\",\"two\",\"three\",\"three-b\"] # list of epochs\n",
    "colors = [\"#c8b4ba\", \"#f3ddb3\", \"#c1cd97\", \"#e18d96\"] # list of colors in hexadecimal format for the epochs\n",
    "markers = [\"o\", \"s\", \"d\", \"v\"] # list of markers for the epochs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# filter and plot the dataset by epochs\n",
    "for (epoch, color, marker) in zip(epochs, colors, markers): # iterate over lists\n",
    "    my_data = my_dataset[my_dataset.Epoch == epoch] # data belonging to epoch\n",
    "    ax.scatter(my_data.Zr, my_data.Th, marker=marker, s=50, \n",
    "               c=color, edgecolor=\"0\", label=\"Epoch \" + epoch) # plot the data\n",
    "\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Th [ppm]\")\n",
    "# ax.grid(True)\n",
    "ax.legend(title = \"Phlegraean Fields \\n Age < 15 ky\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e006da7",
   "metadata": {},
   "source": [
    "In the cell above, we specified the colors using hexadecimal values. These codes can be found using a hexadecimal color picker; [this one](https://g.co/kgs/hpBWRg) for example. The markers can be found from this [link](https://matplotlib.org/stable/api/markers_api.html). \n",
    "\n",
    "Let's save our figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"ZrvsTh.pdf\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d2fb2",
   "metadata": {},
   "source": [
    "That's the end of this notebook. Throughout the course, we will use a lot `pandas`.\n",
    "\n",
    "To practice, try the exercises in `day2/lab/lab2.pdf`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
