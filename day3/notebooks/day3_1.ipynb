{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a690ae",
   "metadata": {},
   "source": [
    "# Day 3-Part 1: Statistics\n",
    "\n",
    "Statistics is a powerful tool to understand or learn from your data. This notebook covers basic concepts of statistics, which will be useful for the second part of the course on machine learning. The material in this notebook comes from the book [Introduction to Python in Earth Science Data Analysis](https://link.springer.com/book/10.1007/978-3-030-78055-5) by Maurizio Petrelli.\n",
    "\n",
    "# Univariate analysis\n",
    "\n",
    "Univariate analysis, also called descriptive statistics, is the statistical analysis of a single variable, which involves qualitative (plots) and quantitative (calculation of parameters) analyses. To illustrate these analyses, we'll use the same dataset we used before, of trace elements concentrations in thepra deposits from a volcano in Italy, and specifically the concentration of Zr. Let's load the dataset and get the Zr concentration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset and read Zr\n",
    "path = os.path.join(\"..\", \"data\", \"Smith_glass_post_NYT_data.xlsx\")\n",
    "my_dataset = pd.read_excel(path, sheet_name=\"Supp_traces\")\n",
    "zr = my_dataset.Zr # Zr concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f047a2b",
   "metadata": {},
   "source": [
    "## Visualizing univariate sample distributions\n",
    "\n",
    "Now let's visualize the distribution of Zr as a histogram of absolute frequencies (counts per bin, plot 1), a histogram of probability density (plot 2), or a cumulative density distribution (plot 3). Notice that the probability density histogram is constructed by dividing the bin's count by the total number of counts and the bin width, so that the area under the histogram integrates to 1. The cumulative density histogram is normalized such that the last bin equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15,4)) # make figure\n",
    "\n",
    "# histogram of absolute frequencies\n",
    "ax[0].hist(zr, bins=20, color=\"#c7ddf4\", edgecolor=\"k\")\n",
    "ax[0].set_xlabel(\"Zr [ppm]\")\n",
    "ax[0].set_ylabel(\"Counts\")\n",
    "ax[0].set_title(\"Plot 1: Absolute frequencies\")\n",
    "\n",
    "# histogram of probability density\n",
    "ax[1].hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\")\n",
    "ax[1].set_xlabel(\"Zr [ppm]\")\n",
    "ax[1].set_ylabel(\"Probability density\")\n",
    "ax[1].set_title(\"Plot 2: Probability density\")\n",
    "\n",
    "# cummulative density distribution\n",
    "ax[2].hist(zr, bins=20, density=True, histtype=\"step\", linewidth=2, cumulative=True)\n",
    "ax[2].set_xlabel(\"Zr [ppm]\")\n",
    "ax[2].set_ylabel(\"Likelihood of occurrence\")\n",
    "ax[2].set_title(\"Plot 3: Cumulative density distribution\")\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59ca60",
   "metadata": {},
   "source": [
    "We can also visualize histograms of all trace elements with just one operation in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms of all traces \n",
    "traces = [\"Sc\",\"Rb\",\"Sr\",\"Y\",\"Zr\",\"Nb\",\"Cs\",\"Ba\",\"La\",\"Ce\",\"Pr\",\"Nd\",\"Sm\",\"Eu\",\n",
    "          \"Gd\",\"Tb\",\"Dy\",\"Ho\",\"Er\",\"Tm\",\"Yb\",\"Lu\",\"Hf\",\"Ta\",\"Pb\",\"Th\",\"U\"]\n",
    "\n",
    "my_dataset[traces].hist(figsize=(17,14), bins=20, color=\"#c7ddf4\", edgecolor=\"k\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a6c93",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "In statistics, it is useful to represent an entire dataset with a single value describing its location. This single value is defined as the central tendency. The mean, median, and mode fall into this category.\n",
    "\n",
    "### Mean\n",
    "\n",
    "The arithmetic mean is the average of all numbers in a dataset:\n",
    "\n",
    "### <div align=\"center\">$\\mu_{\\mathrm{A}}=\\bar{z}=\\frac{1}{n} \\sum_{i=1}^n z_i=\\frac{z_1+z_2+\\cdots+z_n}{n}$</div>\n",
    "\n",
    "The geometric mean indicates the location of a dataset by using the product of their values:\n",
    "\n",
    "### <div align=\"center\">$\\mu_G=\\left(z_1 z_2 \\cdots z_n\\right)^{\\frac{1}{n}}$</div>\n",
    "\n",
    "And the harmonic mean is:\n",
    "\n",
    "### <div align=\"center\">$\\mu_H=\\frac{n}{\\frac{1}{z_1}+\\frac{1}{z_2}+\\cdots+\\frac{1}{z_n}}$</div>\n",
    "\n",
    "The mean can be computed using the `pandas.mean` method. The geometric and harmonic means can be computed using the `scipy.stats.mstats` `gmean` and `hmean` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean, geometric and harmonic means of Zr\n",
    "\n",
    "from scipy.stats.mstats import gmean, hmean\n",
    "\n",
    "a_mean = zr.mean()\n",
    "g_mean = gmean(zr)\n",
    "h_mean = hmean(zr)\n",
    "\n",
    "print (\"arithmetic mean = {:.1f} [ppm]\".format(a_mean))\n",
    "print (\"geometric mean = {:.1f} [ppm]\".format(g_mean))\n",
    "print (\"harmonic mean = {:.1f} [ppm]\".format(h_mean))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(a_mean, color=\"purple\", label=\"Arithmetic mean\", linewidth=2)\n",
    "ax.axvline(g_mean, color=\"orange\", label=\"Geometric mean\", linewidth=2)\n",
    "ax.axvline(h_mean, color=\"green\", label=\"Harmonic mean\", linewidth=2)\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793ac03",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "The median is the number in the middle of a dataset after ordering the data from the lowest to the highest value. If the number of data values is odd, then the median is the middle value in the ordered list; if it is even, then the median is the average of the two middle values. The median can be calculated using the `pandas.median` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute median of Zr\n",
    "\n",
    "median = zr.median()\n",
    "\n",
    "print (\"Median = {:.1f} [ppm]\".format(median))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(median, color=\"orange\", label=\"Median\", linewidth=2)\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea0230",
   "metadata": {},
   "source": [
    "### Mode\n",
    "\n",
    "The mode of a dataset is the value that appears most frequently in the data set. It can be calculated as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mode of Zr\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hist, bin_edges = np.histogram(zr, bins=20, density=True)\n",
    "modal_value = (bin_edges[hist.argmax()] + bin_edges[hist.argmax()+1])/2\n",
    "\n",
    "print (\"Modal value: {:.1f} [ppm]\".format(modal_value))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(modal_value, color=\"orange\", label=\"Modal value\", linewidth=2)\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a1fb4",
   "metadata": {},
   "source": [
    "## Dispersion\n",
    "\n",
    "After having introduced several measures of the central tendency of a dataset, we can now look at measures of its variability. The range, variance, and standard deviation are all estimators of the dispersion (variability) of a dataset.\n",
    "\n",
    "### Range\n",
    "\n",
    "The range is the difference between the highest and lowest values in a dataset:\n",
    "\n",
    "### <div align=\"center\">$R = z_{max} - z_{min}$</div>\n",
    "\n",
    "It can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute range of Zr\n",
    "\n",
    "my_range = zr.max()- zr.min()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(zr.max(), color=\"purple\", label=\"Max value\", linewidth=2)\n",
    "ax.axvline(zr.min(), color=\"green\", label=\"Min value\", linewidth=2)\n",
    "ax.axvspan(zr.min(), zr.max(), alpha=0.1, color=\"orange\", label=\"Range = {:.0f} ppm\".format(my_range))\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edd65e",
   "metadata": {},
   "source": [
    "### Variance and standard deviation\n",
    "\n",
    "The sample variance is defined as:\n",
    "\n",
    "### <div align=\"center\">$\\sigma^2=\\frac{\\sum_{i=1}^n\\left(z_i-\\mu\\right)^2}{n-1}$</div>\n",
    "\n",
    "And the sample standard deviation is the square root of the sample variance:\n",
    "\n",
    "### <div align=\"center\">$\\sigma=\\sqrt{\\sigma^2}=\\sqrt{\\frac{\\sum_{i=1}^n\\left(z_i-\\mu\\right)^2}{n-1}}$</div>\n",
    "\n",
    "The sample variance and standard deviation can be estimated using the `pandas` `var` and `std` methods, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629135c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance and standard deviation of Zr\n",
    "\n",
    "variance = zr.var() # sample variance, for population variance use ddof = 0\n",
    "stddev = zr.std() # sample standard deviation, for population stddev use ddof = 0\n",
    "\n",
    "print (\"Variance = {:.1f} [ppm]\".format(variance))\n",
    "print (\"Standard Deviation = {:.1f} [ppm]\".format(stddev))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins= 20, density = True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(a_mean-stddev, color=\"purple\", label=r\"mean - 1$\\sigma$\", linewidth=2)\n",
    "ax.axvline(a_mean+stddev, color=\"green\", label=r\"mean + 1$\\sigma$\", linewidth=2)\n",
    "ax.axvspan(a_mean-stddev, a_mean+stddev, alpha=0.1, color=\"orange\", label=r\"mean $\\pm$ 1$\\sigma$\")\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823054f",
   "metadata": {},
   "source": [
    "### Inter-quartile range\n",
    "\n",
    "In descriptive statistics, the inter-quartile range is the difference between the 75th and 25th percentiles, or between the third and first quartiles. It can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d964fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute inter-quartile range of Zr\n",
    "\n",
    "# using numpy\n",
    "\n",
    "q1 = np.percentile(zr, 25, interpolation = \"midpoint\") # 1st quartile\n",
    "q3 = np.percentile(zr, 75, interpolation = \"midpoint\") # 2nd quartile\n",
    "\n",
    "# using pandas\n",
    "#q1 = zr.quantile(0.25)\n",
    "#q3 = zr.quantile(0.75)\n",
    "\n",
    "iqr = q3 - q1 # inter-quartile range\n",
    "\n",
    "print (\"Inter-quartile range = {:.1f} [ppm]\".format(iqr))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins= 20, density = True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(q1, color=\"purple\", label=\"Q1\", linewidth=2)\n",
    "ax.axvline(q3, color=\"green\", label=\"Q3\", linewidth=2)\n",
    "ax.axvspan(q1, q3, alpha=0.1, color=\"orange\", label=\"Inter-quartile range (IQR)\")\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50bfbb",
   "metadata": {},
   "source": [
    "A boxplot uses the inter-quartile range to describe groups of numerical data. Lines extending from the boxes (i.e. whiskers) indicate the variability outside the third and first quartiles. Outliers are plotted as individual symbols. In detail, the bottom and top of the box represent the first and third quartiles, respectively. A line is drawn inside the box to represent the second quartile (i.e. the median). To construct a boxplot of Zr concentrations divided by epochs, we can use the `seaborn.boxplot` method as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5558442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxplot for Zn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.boxplot(data=my_dataset, x=\"Zr\", y=\"Epoch\", palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83a241",
   "metadata": {},
   "source": [
    "we can also do this for all trace elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots for all trace elements\n",
    "\n",
    "for trace in traces:\n",
    "    sns.boxplot(data=my_dataset, x=trace, y=\"Epoch\", palette=\"Set3\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfc332",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "\n",
    "Having introduced several parameters providing information about the central tendency and the variability of a dataset, we can now look at the skewness, which reflects the shape of the distribution. \n",
    "\n",
    "The skewness provides information about the symmetry in a distribution of values. In the case of a symmetric distribution, the mean, median, and mode are the same. Conversely, the non-coincidence of these three parameters indicates a skewed distribution.\n",
    "\n",
    "In the case of the concentration distribution of Zr, the mean ($\\mu = \\mu_A$), median ($M_e$), and mode ($M_o$) do not coincide, and a tail appears on the right side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3862f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness of Zr\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(zr, bins=20, density=True, color=\"#c7ddf4\", edgecolor=\"k\", label=\"Measurements Hist\")\n",
    "ax.axvline(a_mean, color=\"green\", label=\"Arithmetic mean\", linewidth=2)\n",
    "ax.axvline(median, color=\"purple\", label=\"Median Value\", linewidth=2)\n",
    "ax.axvline(modal_value, color=\"orange\", label=\"Modal value\", linewidth=2)\n",
    "ax.set_xlabel(\"Zr [ppm]\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e38d5",
   "metadata": {},
   "source": [
    "By comparison with the figure below, this corresponds to a positive skewness:\n",
    "\n",
    "<img src='../figures/skewness.png' style=\"width:600px\" align=\"center\">\n",
    "\n",
    "Another parameter providing information about the skewness of a sample distribution is Pearson's first coefficient of skewness:\n",
    "\n",
    "### <div align=\"center\">$\\alpha_1=\\frac{(\\mu - M_o)}{\\sigma_s}$</div>\n",
    "\n",
    "A second parameter is the Pearson's second moment of skewness:\n",
    "\n",
    "### <div align=\"center\">$\\alpha_2=\\frac{3(\\mu - M_e)}{\\sigma_s}$</div>\n",
    "\n",
    "And a third parameter is the Fisher-Pearson coefficient of skewness:\n",
    "\n",
    "### <div align=\"center\">$g_1=\\frac{m_3}{m_2^{3/2}}$</div>\n",
    "\n",
    "where:\n",
    "\n",
    "### <div align=\"center\">$m_i=\\frac{1}{N} \\sum_{n=1}^N(x[n]-\\mu)^i$</div>\n",
    "\n",
    "In Python, these three parameters can be determined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute skewness Pearson\"s coefficients\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "a1 = (a_mean - modal_value) / stddev\n",
    "a2 = 3 * (a_mean - median) / stddev\n",
    "g1 = skew(zr)\n",
    "\n",
    "print (\"Pearson 1st coeff. of skewness = {:.2f}\".format(a1))\n",
    "print (\"Pearson 2nd moment of skewness = {:.2f}\".format(a2))\n",
    "print (\"Fisher-Pearson coeff. of skewness = {:.2f}\".format(g1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c3d1e",
   "metadata": {},
   "source": [
    "# Bivariate analysis\n",
    "\n",
    "Bivariate statistics investigates the relationships between two variables. Let's look at the concentrations of La, Ce, Sc, and U:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4)) # make figure\n",
    "\n",
    "# plot La vs Ce\n",
    "ax[0].scatter(my_dataset.La, my_dataset.Ce, marker=\"o\", edgecolor=\"k\", color=\"#c7ddf4\")\n",
    "ax[0].set_xlabel(\"La [ppm]\")\n",
    "ax[0].set_ylabel(\"Ce [ppm]\")\n",
    "\n",
    "# plot Sc vs U\n",
    "ax[1].scatter(my_dataset.Sc, my_dataset.U, marker=\"o\", edgecolor=\"k\", color=\"#c7ddf4\")\n",
    "ax[1].set_xlabel(\"Sc [ppm]\")\n",
    "ax[1].set_ylabel(\"U [ppm]\")\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20befc44",
   "metadata": {},
   "source": [
    "Another way to visualize these data is using the `seaborn.pairplot` function. This function plots pairwise relationships from a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairwise relationships for La, Ce, Sc, and U\n",
    "\n",
    "my_sub_dataset = my_dataset[[\"La\",\"Ce\",\"Sc\",\"U\"]] \n",
    "sns.pairplot(my_sub_dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8215cb3",
   "metadata": {},
   "source": [
    "The plot La versus Ce clearly shows an increase in Ce as La increases, while U seems to decrease as Sc increases, although this second correlation is not as clear. To capture the relationships between these variables, we can use the concepts of covariance and correlation.\n",
    "\n",
    "## Covariance and Correlation\n",
    "\n",
    "The covariance of two sets of univariate samples $x$ and $y$ derived from two randoms variables $X$ and $Y$ is a measure of their joint variability, or their degree of correlation:\n",
    "\n",
    "### <div align=\"center\">$\\operatorname{Cov}_{x y}=\\frac{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)\\left(x_i-\\bar{x}\\right)}{n-1}$</div>\n",
    "\n",
    "${Cov}_{xy} > 0$ indicates a positive relationship between $Y$ and $X$. In contrast, if ${Cov}_{xy} < 0$, the relationship is negative. If $X$ and $Y$ are statistically independent, then ${Cov}_{xy} = 0$.\n",
    "\n",
    "Note that the covariance depends on the magnitudes of the two variables inspected. Consequently, it does not tell much about the strength of such relationship. The normalized version of the covariance is the correlation coefficient. This coefficient ranges from -1 to 1 and shows, by its magnitude, the strength of the linear relation. The correlation coefficient $r_{xy}$ for two univariate sets of data, $X$ and $Y$, characterized by a covariance ${Cov}_{xy}$ and standard deviations $\\sigma_{sx}$ and $\\sigma_{sy}$ is:\n",
    "\n",
    "### <div align=\"center\">$r_{x y}=\\frac{\\operatorname{Cov}_{x y}}{\\sigma_{s x} \\sigma_{s y}}=\\frac{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)\\left(x_i-\\bar{x}\\right)}{\\sqrt{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2 \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}}$</div>\n",
    "\n",
    "With `pandas`, the covariance and the correlation coefficient can be easily computed using the `cov`and `corr` functions. These functions calculate the covariance and the correlation matrices for a `DataFrame`, respectively. A covariance matrix is a table showing the covariances ${Cov}_{xy}$ between variables in the `DataFrame`. Each cell in the table shows the covariance between two variables. The correlation matrix follows the same logic, but gives the correlation coefficients. In the correlation matrix, the diagonal cells all contain unity, which corresponds to self-correlation. We can use the `seaborn.heatmap` function to graphically display these matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659deb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot covariance and correlation matrices for La, Ce, Sc, and U\n",
    "\n",
    "cov = my_sub_dataset.cov()\n",
    "cor = my_sub_dataset.corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11,5))\n",
    "\n",
    "ax[0].set_title(\"Covariance Matrix\")\n",
    "sns.heatmap(cov, annot=True, cmap=\"cividis\", ax=ax[0])\n",
    "\n",
    "ax[1].set_title(\"Correlation Matrix\")\n",
    "sns.heatmap(cor, annot=True, vmin= -1, vmax=1, cmap=\"coolwarm\", ax=ax[1])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f288d",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Considering a response variable $Y$ and a predictor $X$, we can define a linear model as follows:\n",
    "\n",
    "### <div align=\"center\">$Y = \\beta_0 + \\beta_1 X + \\epsilon$</div>\n",
    "\n",
    "$\\beta_0$ is the intercept or predicted value of $Y$ at $X = 0$. $\\beta_1$ is the slope or the change in $Y$ per unit change in $X$. The quantity $\\epsilon$ is the residual error. Using the least-squares method (i.e. minimizing the sum of squares of the vertical distances from each point to the line defined by the equation above), $\\beta_1$ and $\\beta_0$ are estimated as:\n",
    "\n",
    "### <div align=\"center\">$\\beta_1=\\frac{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)\\left(x_i-\\bar{x}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}=\\frac{\\operatorname{Cov}_{x y}}{\\sigma_{s x}^2}=r_{x y} \\frac{\\sigma_{s y}}{\\sigma_{s x}}$</div>\n",
    "\n",
    "### <div align=\"center\">$\\beta_0 = \\bar{y} - \\beta_1\\bar{x}$</div>\n",
    "\n",
    "The square of the correlation coefficient, $r^2_{xy}$, is a value between 0 and 1, and it is typically used to make a preliminary estimate of the quality of the regression model. \n",
    "\n",
    "Let's do a linear regression to the La versus Ce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739e96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# linear regression of La versus Ce\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot La vs Ce\n",
    "ax.scatter(my_dataset.La, my_dataset.Ce, marker=\"o\", edgecolor=\"k\", color=\"#c7ddf4\")\n",
    "\n",
    "# linear regression of La vs Ce\n",
    "b1, b0, rho_value, p_value, std_err = linregress(my_dataset.La, my_dataset.Ce)\n",
    "\n",
    "# best-fit line\n",
    "xr = np.linspace(my_dataset[\"La\"].min(),my_dataset.La.max(), 10) # 10 values between min and max La (x values)\n",
    "yr = b0 + b1*xr # 10 Ce (y) values that follow the best-fit line\n",
    "\n",
    "# plot best-fit line and add to the legend b0, b1 and r^2\n",
    "ax.plot(xr, yr, linewidth=1, color=\"#ff464a\", linestyle=\"--\", label=r\"fit param.: $\\beta_0$ = \" \n",
    "            + \"{:.1f}\".format(b0) + r\", $\\beta_1$ = \" + \"{:.1f}\".format(b1) \n",
    "            + r\", $r_{xy}^{2}$ = \" + \"{:.2f}\".format(rho_value**2))\n",
    "\n",
    "ax.set_xlabel(\"La [ppm]\")\n",
    "ax.set_ylabel(\"Ce [ppm]\")\n",
    "ax.legend(loc = \"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5645d",
   "metadata": {},
   "source": [
    "And a linear regression to the Sc versus U data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73008f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression of Sc versus U\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot Sc vs U\n",
    "ax.scatter(my_dataset.Sc, my_dataset.U, marker=\"o\", edgecolor=\"k\", color=\"#c7ddf4\")\n",
    "\n",
    "# linear regresssion of Sc vs U\n",
    "b1, b0, rho_value, p_value, std_err = linregress(my_dataset.Sc, my_dataset.U)\n",
    "\n",
    "# best-fit line\n",
    "xr = np.linspace(my_dataset.Sc.min(),my_dataset.Sc.max(), 10) # 10 values between min and max Sc (x values)\n",
    "yr = b0 + b1*xr # 10 U (y) values that follow the best-fit line\n",
    "\n",
    "# plot best-fit line and add to the legend b0, b1 and r^2\n",
    "ax.plot(xr, yr, linewidth=1, color=\"#ff464a\", linestyle=\"--\", label=r\"fit param.: $\\beta_0$ = \" \n",
    "            + \"{:.1f}\".format(b0) + r\", $\\beta_1$ = \" + \"{:.1f}\".format(b1) \n",
    "            + r\", $r_{xy}^{2}$ = \" + \"{:.2f}\".format(rho_value**2))\n",
    "\n",
    "ax.set_xlabel(\"Sc [ppm]\")\n",
    "ax.set_ylabel(\"U [ppm]\")\n",
    "ax.legend(loc = \"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a58e3e",
   "metadata": {},
   "source": [
    "So as we concluded by visual inspection before, the correlation is stronger for La versus Ce than for Sc versus U.\n",
    "\n",
    "\n",
    "# Probability Density Functions\n",
    "\n",
    "A probability density function (PDF) is a function associated with a continuous random variable whose value at any point in the sample space (i.e. the possible set of values for the random variable) is an estimate of the likelihood of occurrence of that event. \n",
    "\n",
    "## The Normal Distribution\n",
    "\n",
    "The normal distribution is a bell-shaped PDF that occurs in many situations. The normal PDF is defined as follows:\n",
    "\n",
    "### <div align=\"center\">$\\operatorname{PDF}_{\\mathrm{N}}(x, \\mu, \\sigma)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}$</div>\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and the standard deviation, respectively.\n",
    "\n",
    "The `scipy.stats.norm.pdf` function provides the PDF for a normal distribution. In the code below, we also use the `numpy.random.normal` function to generate a random sample from a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal distribution\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 0 #mean\n",
    "sigma = 1 # standard deviation\n",
    "normal_sample = np.random.normal(mu, sigma, 15000) # random sample from normal distribution\n",
    "\n",
    "# plot the histogram of the sample distribution\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(normal_sample, bins=\"auto\", density=True, color=\"#c7ddf4\", edgecolor=\"k\",\n",
    "        label=\"Random sample with normal distribution\")\n",
    "\n",
    "# probability density function\n",
    "x = np.arange(-5,5, 0.01)\n",
    "normal_pdf = norm.pdf(x, loc= mu, scale = sigma)\n",
    "ax.plot(x, normal_pdf, color=\"#ff464a\", linewidth=1.5, linestyle=\"--\",\n",
    "        label=r\"Normal PDF with $\\mu$=0 and $\\sigma$=1\")\n",
    "ax.legend(title=\"Normal Distribution\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_xlim(-5,5)\n",
    "ax.set_ylim(0,0.6);\n",
    "\n",
    "# Descriptive statistics\n",
    "s_mean = normal_sample.mean()\n",
    "s_stddev = normal_sample.std()\n",
    "print(\"Sample mean = {:.4f}\".format(s_mean))\n",
    "print(\"Sample standard deviation = {:.4f}\".format(s_stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d1d28",
   "metadata": {},
   "source": [
    "## The Log-Normal Distribution\n",
    "\n",
    "The log-normal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. The PDF for a log-normal distribution is:\n",
    "\n",
    "### <div align=\"center\">$\\operatorname{logPDF}_{\\mathrm{N}}\\left(x, \\mu_n, \\sigma_n\\right)=\\frac{1}{x} \\frac{1}{\\sqrt{2 \\pi \\sigma_n^2}} \\exp \\left\\{-\\frac{\\left[\\log (x)-\\mu_n\\right]^2}{2 \\sigma_n^2}\\right\\}$</div>\n",
    "\n",
    "where $\\mu_n$ and $\\sigma_n$ are the mean and the standard deviation of the normal distribution and are obtained by calculating the natural logarithm (log) of the random variable.\n",
    "\n",
    "The `scipy.stats.lognorm.pdf` function provides the PDF for a log-normal distribution, and the `scipy.stats.lognorm.rvs` function generates random numbers following the log-normal distribution. The code below generates random samples with log-normal distributions for several values of $\\mu_n$ and $\\sigma_n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log normal distribution\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "colors = [\"#342a77\", \"#ff464a\", \"#4881e9\"]\n",
    "normal_mu = [0,0.5,1]\n",
    "normal_sigma = [0.5,0.4,0.3]\n",
    "x = np.arange(0.001, 7, .001) # for the log-normal PDF \n",
    "x1 = np.arange(-2.5, 2.5, .001) # for the normal PDF\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,4))\n",
    "\n",
    "for mu_n, sigma_n, color in zip(normal_mu, normal_sigma, colors):\n",
    "    \n",
    "    # log-normal distribution of x\n",
    "    lognorm_pdf = lognorm.pdf(x, s=sigma_n, scale=np.exp(mu_n)) # pdf\n",
    "    r = lognorm.rvs(s=sigma_n, scale=np.exp(mu_n), size=15000) # random sample\n",
    "    ax[0].plot(x, lognorm_pdf, color=color, label=r\"$\\mu_n$ =\" + str(mu_n) + r\"- $\\sigma_n$ =\" + str(sigma_n))\n",
    "    ax[0].hist(r, bins=\"auto\", density=True, color=color, edgecolor=\"k\", alpha=0.5)\n",
    "    \n",
    "    # normal distribution of log(x)\n",
    "    normal_pdf = norm.pdf(x1, loc= mu_n, scale = sigma_n) # pdf\n",
    "    logr= np.log(r) # random sample\n",
    "    ax[1].plot(x1, normal_pdf, color=color, label=r\"$\\mu_n$ =\" + str(mu_n) + r\"- $\\sigma_n$ =\" + str(sigma_n))\n",
    "    ax[1].hist(logr, bins=\"auto\", density=True, color=color, edgecolor=\"k\", alpha=0.5)\n",
    "    my_mu = logr.mean()\n",
    "    ax[1].axvline(x=my_mu, color=color, linestyle=\"--\", label=r\"calculated $\\mu_n$ =\" + str(round(my_mu,3)))\n",
    "    \n",
    "ax[0].legend(title=\"log-normal distributions\") \n",
    "ax[0].set_xlabel(\"x\") \n",
    "ax[0].set_ylabel(\"Probability Density\") \n",
    "ax[1].legend(title=\"normal distributions\") \n",
    "ax[1].set_xlabel(\"ln(x)\") \n",
    "ax[1].set_ylabel(\"Probability Density\")\n",
    "\n",
    "fig.tight_layout(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c59f39",
   "metadata": {},
   "source": [
    "The [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) module has many other probability distributions, that are useful in geological applications.\n",
    "\n",
    "\n",
    "## Kernel density estimators\n",
    "\n",
    "Kernel density estimators (KDE) are used to estimate a PDF from the observed data. Assume $(x_1, x_2, x_i, ..., x_n)$ is a univariate, independent, and identically distributed sample (i.e. all $x_i$ have the same probability distribution) belonging to a distribution with unknown PDF. We are interested in estimating the shape $\\hat{f}$ of this PDF. The equation defining a KDE is:\n",
    "\n",
    "### <div align=\"center\">$\\hat{f}(x)=\\frac{1}{n h} \\sum_{i=1}^n K\\left(\\frac{x-x(i)}{h}\\right)$</div>\n",
    "\n",
    "where $K$ is the kernel, a non-negative function that integrates to unity (i.e. $\\int_{-\\infty}^{\\infty} K(x) d x=1$), and $h>0$ is a smoothing parameter called the bandwidth. A range of kernel functions are commonly used: normal, uniform, triangular, etc. \n",
    "\n",
    "Python offers several different implementations for the development of a KDE. The code below plots the kernel functions available in the `statsmodel` package, `KDEUnivariate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot kernel functions\n",
    "\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate \n",
    "\n",
    "kernels = [\"gau\", \"epa\", \"uni\", \"tri\", \"biw\", \"triw\"] # kernel functions\n",
    "kernels_names = [\"Gaussian\", \"Epanechnikov\", \"Uniform\", \"Triangular\", \"Biweight\", \"Triweight\"] \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for kernel, kernel_name in zip(kernels, kernels_names):\n",
    "    # kernels\n",
    "    kde = KDEUnivariate([0]) # kernel at x = 0\n",
    "    kde.fit(kernel= kernel, bw=1, fft=False, gridsize=2**10) \n",
    "    ax.plot(kde.support, kde.density, label = kernel_name, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlim(-2,2)\n",
    "ax.grid()\n",
    "ax.legend(title=\"Kernel functions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f3a8c",
   "metadata": {},
   "source": [
    "The code below shows the application of KDE to the data of Zr concentrations, and how the bandwidth affects the resulting KDE estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b05f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the kernel to Zr concentrations\n",
    "\n",
    "zr_eval = np.arange(0, 1100, 1)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(7,8))\n",
    "\n",
    "# Density Histogram\n",
    "ax[0].hist(zr, bins=20, density=True, label=\"Density Histogram\", color=\"#c7ddf4\", edgecolor=\"k\")\n",
    "# KDE\n",
    "kde = KDEUnivariate(zr)\n",
    "kde.fit() # This uses the Gaussian kernel and normal_reference bandwidth\n",
    "my_kde = kde.evaluate(zr_eval)\n",
    "ax[0].plot(zr_eval, my_kde, linewidth=1.5, color=\"#ff464a\", label=\"gaussian KDE - auto bandwidth selection\") \n",
    "ax[0].set_xlabel(\"Zr [ppm]\") \n",
    "ax[0].set_ylabel(\"Probability density\") \n",
    "ax[0].set_ylim(0, 0.006)\n",
    "ax[0].legend();\n",
    "\n",
    "# Density Histogram\n",
    "ax[1].hist(zr, bins=20, density=True, label=\"Density Histogram\", color=\"#c7ddf4\", edgecolor=\"k\")\n",
    "# KDE\n",
    "# Effect of bandwidth\n",
    "for my_bw in [10,50,100]:\n",
    "    kde = KDEUnivariate(zr)\n",
    "    kde.fit(bw = my_bw)\n",
    "    my_kde = kde.evaluate(zr_eval)\n",
    "    ax[1].plot(zr_eval, my_kde, linewidth = 1.5, label=\"gaussian KDE - bw: \" + str(my_bw))\n",
    "    \n",
    "ax[1].set_xlabel(\"Zr [ppm]\") \n",
    "ax[1].set_ylabel(\"Probability density\") \n",
    "ax[1].set_ylim(0, 0.006)\n",
    "ax[1].legend()\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f5740",
   "metadata": {},
   "source": [
    "That's it. In the next notebook, we will look at uncertainties or errors and their propagation.\n",
    "\n",
    "To practice, try the exercise in [lab3_1](../lab/lab3_1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
